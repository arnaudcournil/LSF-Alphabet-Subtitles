{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\courn\\Desktop\\Projet ML\\LSF-Alphabet-Subtitles\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "c:\\Users\\courn\\Desktop\\Projet ML\\LSF-Alphabet-Subtitles\\.venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"dictionnaire.txt\", 'r', encoding='latin1') as file1:\n",
    "    with open(\"lexiqueorgdico.txt\", 'r', encoding='latin1') as file2:\n",
    "        file = list(set(file1).union(set(file2)))\n",
    "        french_words = [unidecode(word.lower().strip()) for word in file if all(c in \"abcdefghijklmnopqrstuvwxyzéèàçù\" for c in word.strip().lower())]\n",
    "\n",
    "def capitalize_sentences(text):\n",
    "    # Utilisation de re.sub pour identifier chaque phrase et mettre la première lettre en majuscule\n",
    "    return re.sub(r'(^|(?<=[.!?…])\\s+)([a-z])', lambda match: match.group(1) + match.group(2).upper(), text)\n",
    "\n",
    "model = PunctuationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['j', 'aime', 'let'], 'empslactionlamertume')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxMatch(string):\n",
    "    tokens = []\n",
    "    not_in = \"\"\n",
    "    i = 0\n",
    "    while i < len(string):\n",
    "        maxWord = \"\"\n",
    "        for j in range(i, len(string)):\n",
    "            tempWord = string[i:j+1]\n",
    "            if tempWord in french_words and len(tempWord) > len(maxWord):\n",
    "                maxWord = tempWord\n",
    "        if len(maxWord) == 0:\n",
    "            not_in = string[i:j+1]\n",
    "            break\n",
    "        i = i+len(maxWord)\n",
    "        tokens.append(maxWord)\n",
    "    return tokens, not_in\n",
    "\n",
    "string = \"jaimeletempslactionlamertume\"\n",
    "maxMatch(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['j',\n",
       "  'aime',\n",
       "  'let',\n",
       "  'e',\n",
       "  'm',\n",
       "  'p',\n",
       "  's',\n",
       "  'lac',\n",
       "  't',\n",
       "  'ion',\n",
       "  'lamer',\n",
       "  'tu',\n",
       "  'me'],\n",
       " [0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def completeMaxMatch(string):\n",
    "    tokens, not_in = maxMatch(string)\n",
    "    not_in_array = [0] * len(tokens)\n",
    "    while len(not_in) > 0:\n",
    "        tokens.append(not_in[0])\n",
    "        not_in_array.append(1)\n",
    "        not_in  = not_in[1:]\n",
    "        if len(not_in) > 0:\n",
    "            tokens_, not_in = maxMatch(not_in)\n",
    "            tokens.extend(tokens_)\n",
    "            not_in_array.extend([0] * len(tokens_))\n",
    "    return  tokens, not_in_array\n",
    "\n",
    "string = \"jaimeletempslactionlamertume\"\n",
    "completeMaxMatch(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['amertume'], 'jaimeletempslactionl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverseMaxMatch(string):\n",
    "    tokens = []\n",
    "    not_in = \"\"\n",
    "    i = len(string)\n",
    "    while i > 0:\n",
    "        maxWord = \"\"\n",
    "        for j in range(i - 1, -1, -1):\n",
    "            tempWord = string[j:i]\n",
    "            if tempWord in french_words and len(tempWord) > len(maxWord):\n",
    "                maxWord = tempWord\n",
    "        if len(maxWord) == 0:\n",
    "            not_in = string[j:i]\n",
    "            break\n",
    "        i = i - len(maxWord)\n",
    "        tokens.append(maxWord)\n",
    "    return tokens[::-1], not_in\n",
    "\n",
    "string = \"jaimeletempslactionlamertume\"\n",
    "reverseMaxMatch(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['j', 'aime', 'le', 'temps', 'l', 'action', 'l', 'amertume'],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def completeReverseMaxMatch(string):\n",
    "    tokens, not_in = reverseMaxMatch(string)\n",
    "    not_in_array = [0] * len(tokens)\n",
    "    while len(not_in) > 0:\n",
    "        tokens.insert(0, not_in[-1])\n",
    "        not_in_array.insert(0, 1)\n",
    "        not_in  = not_in[:-1]\n",
    "        if len(not_in) > 0:\n",
    "            tokens_, not_in = reverseMaxMatch(not_in)\n",
    "            tokens = tokens_ + tokens \n",
    "            not_in_array = [0] * len(tokens_) + not_in_array\n",
    "    return tokens, not_in_array\n",
    "\n",
    "string = \"jaimeletempslactionlamertume\"\n",
    "completeReverseMaxMatch(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_idx(all_tokens_l):\n",
    "    l_splits_idx = []\n",
    "    sum_ = 0\n",
    "    for token in all_tokens_l:\n",
    "        sum_ += len(token)\n",
    "        l_splits_idx.append(sum_)\n",
    "    return l_splits_idx\n",
    "\n",
    "def compare_tokens_list(all_tokens_l1, not_in_array_l1, all_tokens_l2, not_in_array_l2):\n",
    "    if sum(not_in_array_l1) <  sum(not_in_array_l2):\n",
    "        return all_tokens_l1\n",
    "    elif sum(not_in_array_l1) > sum(not_in_array_l2):\n",
    "        return all_tokens_l2\n",
    "    else:\n",
    "        sum_square_len_l1 = sum([len(word) ** 2 for word in all_tokens_l1])\n",
    "        sum_square_len_l2 = sum([len(word) ** 2 for word in all_tokens_l2])\n",
    "        return all_tokens_l1 if sum_square_len_l1 > sum_square_len_l2 else all_tokens_l2\n",
    "\n",
    "def mix_algo(string):\n",
    "    tokens_max, not_in_array_max = completeMaxMatch(string)\n",
    "    tokens_reverse_max, not_in_array_reverse_max = completeReverseMaxMatch(string)\n",
    "\n",
    "    max_splits_idx = get_splits_idx(tokens_max)\n",
    "    reverse_max_splits_idx = get_splits_idx(tokens_reverse_max)\n",
    "    \n",
    "    finals_words = []\n",
    "    i_act = 0\n",
    "    j_act = 0\n",
    "    for i in range(len(max_splits_idx)):\n",
    "        for j in range(j_act, len(reverse_max_splits_idx)):\n",
    "            if max_splits_idx[i] == reverse_max_splits_idx[j]:\n",
    "                finals_words.extend(compare_tokens_list(tokens_max[i_act : i + 1], not_in_array_max[i_act : i + 1], tokens_reverse_max[j_act : j + 1], not_in_array_reverse_max[j_act : j + 1]))\n",
    "                i_act = i + 1\n",
    "                j_act = j + 1\n",
    "                break\n",
    "    return finals_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_optimise(string):\n",
    "    MAX_WORD_LENGTH = 25\n",
    "    confirmed_ = []\n",
    "    next_ = []\n",
    "    next_raw  = []\n",
    "    i = 0\n",
    "    while True :\n",
    "        next_ = mix_algo(string[i:i+MAX_WORD_LENGTH])\n",
    "        if i + MAX_WORD_LENGTH >= len(string):\n",
    "            next_raw = string[i:i+MAX_WORD_LENGTH]\n",
    "            break\n",
    "        else:\n",
    "            confirmed_.append(next_[0])\n",
    "            i += len(next_[0])\n",
    "        \n",
    "    return confirmed_, next_, next_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3642868995666504\n",
      "0.08113861083984375\n",
      "0.09110474586486816\n",
      "0.09232091903686523\n",
      "0.10866689682006836\n",
      "0.10566186904907227\n",
      "0.12026357650756836\n",
      "0.1308591365814209\n",
      "0.13041353225708008\n",
      "0.16025900840759277\n",
      "0.17586255073547363\n",
      "0.18616628646850586\n",
      "0.18634653091430664\n",
      "0.19272351264953613\n",
      "0.22929668426513672\n",
      "0.24340486526489258\n",
      "0.27930283546447754\n",
      "0.25632190704345703\n",
      "0.2666773796081543\n",
      "0.33224964141845703\n",
      "0.3291130065917969\n",
      "0.3187687397003174\n",
      "0.3187122344970703\n",
      "0.3712291717529297\n",
      "0.43076109886169434\n",
      "0.39023470878601074\n",
      "0.5013022422790527\n",
      "0.6316475868225098\n",
      "0.576901912689209\n",
      "0.503044843673706\n",
      "0.49495625495910645\n",
      "0.5639760494232178\n",
      "0.646714448928833\n",
      "0.7658421993255615\n",
      "0.7108008861541748\n",
      "0.7101924419403076\n",
      "0.756040096282959\n",
      "0.6435153484344482\n",
      "0.6588184833526611\n",
      "0.7362382411956787\n",
      "0.8151490688323975\n",
      "0.7497823238372803\n",
      "0.851689338684082\n",
      "0.8791098594665527\n",
      "0.8757216930389404\n",
      "0.8282585144042969\n",
      "0.8694193363189697\n",
      "0.8721764087677002\n",
      "0.9860200881958008\n",
      "1.2975318431854248\n",
      "1.1001019477844238\n",
      "1.0887763500213623\n",
      "1.036961555480957\n",
      "1.084876537322998\n",
      "1.274656057357788\n",
      "1.3392670154571533\n",
      "1.3590874671936035\n",
      "1.3522114753723145\n",
      "1.5137758255004883\n",
      "1.591783046722412\n",
      "1.7283611297607422\n",
      "1.5754895210266113\n",
      "1.825484037399292\n",
      "1.9255154132843018\n",
      "1.8087489604949951\n",
      "1.8680992126464844\n",
      "2.018789529800415\n",
      "2.0666182041168213\n",
      "2.234710693359375\n",
      "1.9326958656311035\n",
      "1.8382723331451416\n",
      "1.884394645690918\n",
      "1.968109130859375\n",
      "2.0565359592437744\n",
      "1.9793798923492432\n",
      "1.9350101947784424\n",
      "2.0781517028808594\n",
      "2.321221113204956\n",
      "2.258362054824829\n",
      "2.244370222091675\n",
      "2.2453794479370117\n",
      "2.4893665313720703\n",
      "2.836336374282837\n",
      "Le ciel est bleu. J aides amis qui sont aussi mes amoureux et des chemins qui sont aussi un peul es miens.\n"
     ]
    }
   ],
   "source": [
    "all_keys = \"lecielestbleujaidesamisquisontaussimesamoureuxetdescheminsquisontaussiunpeulesmiens\"\n",
    "next_raw = \"\"\n",
    "confirmed  = []\n",
    "\n",
    "for key in all_keys:\n",
    "    next_raw += key # Ajout d'une lettre\n",
    "    start = time.time()\n",
    "    confirmed = mix_algo(next_raw)\n",
    "    result = re.sub(r\"\\b([JjLlCc]) (\\w+)\", r\"\\1'\\2\", capitalize_sentences(model.restore_punctuation(\" \".join(confirmed))))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10761380195617676\n",
      "0.09238553047180176\n",
      "0.10279297828674316\n",
      "0.09963607788085938\n",
      "0.12524747848510742\n",
      "0.12404990196228027\n",
      "0.13698601722717285\n",
      "0.13226938247680664\n",
      "0.12989425659179688\n",
      "0.15300726890563965\n",
      "0.1855030059814453\n",
      "0.21556448936462402\n",
      "0.2089405059814453\n",
      "0.23837542533874512\n",
      "0.2292780876159668\n",
      "0.2295215129852295\n",
      "0.26164746284484863\n",
      "0.24991965293884277\n",
      "0.27812981605529785\n",
      "0.3737657070159912\n",
      "0.479999303817749\n",
      "0.43828463554382324\n",
      "0.4238002300262451\n",
      "0.4588890075683594\n",
      "0.40859556198120117\n",
      "0.677424430847168\n",
      "0.3771078586578369\n",
      "0.6629929542541504\n",
      "0.36408019065856934\n",
      "0.33022522926330566\n",
      "0.41300034523010254\n",
      "0.6243979930877686\n",
      "0.40465760231018066\n",
      "0.4186713695526123\n",
      "0.7170510292053223\n",
      "0.39542722702026367\n",
      "0.34298133850097656\n",
      "0.39655590057373047\n",
      "0.534895658493042\n",
      "0.43728184700012207\n",
      "0.2968776226043701\n",
      "0.39327406883239746\n",
      "0.35272717475891113\n",
      "0.3894221782684326\n",
      "0.5351588726043701\n",
      "0.35786938667297363\n",
      "0.3358163833618164\n",
      "0.3562591075897217\n",
      "0.5050718784332275\n",
      "0.3605508804321289\n",
      "0.43928027153015137\n",
      "0.49681758880615234\n",
      "0.30141186714172363\n",
      "0.33127307891845703\n",
      "0.3787555694580078\n",
      "0.6266834735870361\n",
      "0.2878382205963135\n",
      "0.28644394874572754\n",
      "0.32387280464172363\n",
      "0.3677818775177002\n",
      "0.5161178112030029\n",
      "0.30906152725219727\n",
      "0.33427929878234863\n",
      "0.4769308567047119\n",
      "0.23013806343078613\n",
      "0.2695353031158447\n",
      "0.2779233455657959\n",
      "0.3143134117126465\n",
      "0.35080671310424805\n",
      "0.2936382293701172\n",
      "0.3352329730987549\n",
      "0.50282883644104\n",
      "0.32816648483276367\n",
      "0.5315032005310059\n",
      "0.31223177909851074\n",
      "0.3105778694152832\n",
      "0.5122947692871094\n",
      "0.27841830253601074\n",
      "0.31581902503967285\n",
      "0.3252706527709961\n",
      "0.3772578239440918\n",
      "0.3881855010986328\n",
      "0.36133670806884766\n",
      "Le ciel est bleu. J aides amis qui sont aussi mes amoureux et des chemins qui sont aussi un peul es miens.\n"
     ]
    }
   ],
   "source": [
    "all_keys = \"lecielestbleujaidesamisquisontaussimesamoureuxetdescheminsquisontaussiunpeulesmiens\"\n",
    "next_raw = \"\"\n",
    "next_ = []\n",
    "confirmed  = []\n",
    "\n",
    "for key in all_keys:\n",
    "    next_raw += key # Ajout d'une lettre\n",
    "    start = time.time()\n",
    "    confirmed_, next_, next_raw = algo_optimise(next_raw)\n",
    "    result = re.sub(r\"\\b([JjLlCc]) (\\w+)\", r\"\\1'\\2\", capitalize_sentences(model.restore_punctuation(\" \".join(confirmed + next_))))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    confirmed.extend(confirmed_)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_texte(texte):\n",
    "    # Enlever les accents\n",
    "    texte = unidecode(texte)\n",
    "    # Remplacer les apostrophes par un espace\n",
    "    texte = texte.replace(\"'\", \" \")\n",
    "    # Enlever la ponctuation\n",
    "    texte = re.sub(r\"[^\\w\\s]\", \"\", texte)\n",
    "    # Mettre en minuscule\n",
    "    texte = texte.lower()\n",
    "    # Enlever les espaces en trop\n",
    "    return texte.strip()\n",
    "\n",
    "with open(\"phrases_francaises.txt\", 'r', encoding='utf8') as file:\n",
    "    texte = file.read()\n",
    "sentences =  re.split(r\"[.?!;…,]\", texte.replace(\"\\n\", \" \"))\n",
    "sentences_clean = [nettoyer_texte(sentence) for sentence in sentences if sentence !=  \"\"]\n",
    "\n",
    "df = pd.DataFrame(sentences_clean, columns=[\"phrase_espace\"])\n",
    "df[\"phrase_concat\"]  = df[\"phrase_espace\"].apply(lambda x: x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_espace</th>\n",
       "      <th>phrase_concat</th>\n",
       "      <th>pred_mix_algo</th>\n",
       "      <th>relative_accuracy_mix_algo</th>\n",
       "      <th>pred_max_match</th>\n",
       "      <th>relative_accuracy_max_match</th>\n",
       "      <th>pred_reverse_max_match</th>\n",
       "      <th>relative_accuracy_reverse_max_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les representants du peuple francais</td>\n",
       "      <td>lesrepresentantsdupeuplefrancais</td>\n",
       "      <td>les r e p r es entant s du peuple franc ais</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>les r e p r es entant s dupe u p le franc ais</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>les r e p r e sentant s du peuple franc ais</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constitues en assemblee nationale</td>\n",
       "      <td>constituesenassembleenationale</td>\n",
       "      <td>constitues en assemble e nationale</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>constitues en assemble en a t ion ale</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>constitue sen assemble e nationale</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>considerant que l ignorance</td>\n",
       "      <td>considerantquelignorance</td>\n",
       "      <td>con si der an t quel ignorance</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>cons ide ra n t quel ignorance</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>con si der an t quel ignorance</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l oubli ou le mepris des droits de l homme son...</td>\n",
       "      <td>loublioulemeprisdesdroitsdelhommesontlesseules...</td>\n",
       "      <td>l oubli ou le me pris des droits de l hommes o...</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>l oubli ou le me pris des droits de l hommes o...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>l ou b l ioule me pris des droits de l homme s...</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ont resolu d exposer</td>\n",
       "      <td>ontresoludexposer</td>\n",
       "      <td>ont r es o lu d exposer</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>ont r es o lu de x poser</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>ont r es o lu d exposer</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>je trouve que la television est tres favorable...</td>\n",
       "      <td>jetrouvequelatelevisionesttresfavorablealaculture</td>\n",
       "      <td>je trouve quel a te le vision est t r es favor...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>jet r ou v e quel a tel e vision est t r es fa...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>je trouve que la te le vision est t r es favor...</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>chaque fois que quelqu un l allume chez moi</td>\n",
       "      <td>chaquefoisquequelquunlallumechezmoi</td>\n",
       "      <td>chaque fois que quel q u un l allume chez moi</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>chaque fois que quel q u un la l lu mec h e z moi</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>chaque fois que quel q u un l allume chez moi</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>je vais dans la piece d a cote et je lis un livre</td>\n",
       "      <td>jevaisdanslapiecedacoteetjelisunlivre</td>\n",
       "      <td>je vais dans la pie ce d a cote et je lis un l...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>je vais dans la pie ce d a cote et je lis un l...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>je vais dans la pie ce d a cote et je lis un l...</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>ce sont de nombreux passages de plusieurs page...</td>\n",
       "      <td>cesontdenombreuxpassagesdeplusieurspagesquimon...</td>\n",
       "      <td>ce sont de nombreux passages de plusieurs page...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>ces ont de nombreux passages de plusieurs page...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>ce sont de nombreux passages de plusieurs page...</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>je pourrais aussi bien recopier tout le livre ici</td>\n",
       "      <td>jepourraisaussibienrecopiertoutlelivreici</td>\n",
       "      <td>je pourrais aussi bien recopier tout le livre ici</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>je pourrais aussi bien recopier tout le livre ici</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>je pourrais aussi bien recopier tout le livre ici</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         phrase_espace  \\\n",
       "0                 les representants du peuple francais   \n",
       "1                    constitues en assemblee nationale   \n",
       "2                          considerant que l ignorance   \n",
       "3    l oubli ou le mepris des droits de l homme son...   \n",
       "4                                 ont resolu d exposer   \n",
       "..                                                 ...   \n",
       "118  je trouve que la television est tres favorable...   \n",
       "119        chaque fois que quelqu un l allume chez moi   \n",
       "120  je vais dans la piece d a cote et je lis un livre   \n",
       "121  ce sont de nombreux passages de plusieurs page...   \n",
       "122  je pourrais aussi bien recopier tout le livre ici   \n",
       "\n",
       "                                         phrase_concat  \\\n",
       "0                     lesrepresentantsdupeuplefrancais   \n",
       "1                       constituesenassembleenationale   \n",
       "2                             considerantquelignorance   \n",
       "3    loublioulemeprisdesdroitsdelhommesontlesseules...   \n",
       "4                                    ontresoludexposer   \n",
       "..                                                 ...   \n",
       "118  jetrouvequelatelevisionesttresfavorablealaculture   \n",
       "119                chaquefoisquequelquunlallumechezmoi   \n",
       "120              jevaisdanslapiecedacoteetjelisunlivre   \n",
       "121  cesontdenombreuxpassagesdeplusieurspagesquimon...   \n",
       "122          jepourraisaussibienrecopiertoutlelivreici   \n",
       "\n",
       "                                         pred_mix_algo  \\\n",
       "0          les r e p r es entant s du peuple franc ais   \n",
       "1                   constitues en assemble e nationale   \n",
       "2                       con si der an t quel ignorance   \n",
       "3    l oubli ou le me pris des droits de l hommes o...   \n",
       "4                              ont r es o lu d exposer   \n",
       "..                                                 ...   \n",
       "118  je trouve quel a te le vision est t r es favor...   \n",
       "119      chaque fois que quel q u un l allume chez moi   \n",
       "120  je vais dans la pie ce d a cote et je lis un l...   \n",
       "121  ce sont de nombreux passages de plusieurs page...   \n",
       "122  je pourrais aussi bien recopier tout le livre ici   \n",
       "\n",
       "     relative_accuracy_mix_algo  \\\n",
       "0                      0.600000   \n",
       "1                      0.750000   \n",
       "2                      0.250000   \n",
       "3                      0.789474   \n",
       "4                      0.750000   \n",
       "..                          ...   \n",
       "118                    0.700000   \n",
       "119                    0.888889   \n",
       "120                    0.916667   \n",
       "121                    0.846154   \n",
       "122                    1.000000   \n",
       "\n",
       "                                        pred_max_match  \\\n",
       "0        les r e p r es entant s dupe u p le franc ais   \n",
       "1                constitues en assemble en a t ion ale   \n",
       "2                       cons ide ra n t quel ignorance   \n",
       "3    l oubli ou le me pris des droits de l hommes o...   \n",
       "4                             ont r es o lu de x poser   \n",
       "..                                                 ...   \n",
       "118  jet r ou v e quel a tel e vision est t r es fa...   \n",
       "119  chaque fois que quel q u un la l lu mec h e z moi   \n",
       "120  je vais dans la pie ce d a cote et je lis un l...   \n",
       "121  ces ont de nombreux passages de plusieurs page...   \n",
       "122  je pourrais aussi bien recopier tout le livre ici   \n",
       "\n",
       "     relative_accuracy_max_match  \\\n",
       "0                       0.200000   \n",
       "1                       0.500000   \n",
       "2                       0.250000   \n",
       "3                       0.736842   \n",
       "4                       0.250000   \n",
       "..                           ...   \n",
       "118                     0.300000   \n",
       "119                     0.666667   \n",
       "120                     0.916667   \n",
       "121                     0.846154   \n",
       "122                     1.000000   \n",
       "\n",
       "                                pred_reverse_max_match  \\\n",
       "0          les r e p r e sentant s du peuple franc ais   \n",
       "1                   constitue sen assemble e nationale   \n",
       "2                       con si der an t quel ignorance   \n",
       "3    l ou b l ioule me pris des droits de l homme s...   \n",
       "4                              ont r es o lu d exposer   \n",
       "..                                                 ...   \n",
       "118  je trouve que la te le vision est t r es favor...   \n",
       "119      chaque fois que quel q u un l allume chez moi   \n",
       "120  je vais dans la pie ce d a cote et je lis un l...   \n",
       "121  ce sont de nombreux passages de plusieurs page...   \n",
       "122  je pourrais aussi bien recopier tout le livre ici   \n",
       "\n",
       "     relative_accuracy_reverse_max_match  \n",
       "0                               0.600000  \n",
       "1                               0.250000  \n",
       "2                               0.250000  \n",
       "3                               0.684211  \n",
       "4                               0.750000  \n",
       "..                                   ...  \n",
       "118                             0.800000  \n",
       "119                             0.888889  \n",
       "120                             0.916667  \n",
       "121                             0.846154  \n",
       "122                             1.000000  \n",
       "\n",
       "[123 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    real_tokens = row[\"phrase_espace\"].split()\n",
    "    tokens_mix_algo = mix_algo(row[\"phrase_concat\"])\n",
    "    pred_mix_algo = \" \".join(tokens_mix_algo)\n",
    "    df.at[index, \"pred_mix_algo\"] = pred_mix_algo\n",
    "    relative_accuracy_mix_algo = len(set(real_tokens).intersection(set(tokens_mix_algo))) / len(set(real_tokens))\n",
    "    df.at[index, \"relative_accuracy_mix_algo\"] = relative_accuracy_mix_algo\n",
    "    tokens_max_match = completeMaxMatch(row[\"phrase_concat\"])[0]\n",
    "    pred_max_match = \" \".join(tokens_max_match)\n",
    "    df.at[index, \"pred_max_match\"] = pred_max_match\n",
    "    relative_accuracy_max_match = len(set(real_tokens).intersection(set(tokens_max_match))) / len(set(real_tokens))\n",
    "    df.at[index, \"relative_accuracy_max_match\"] = relative_accuracy_max_match\n",
    "    tokens_reverse_max_match = completeReverseMaxMatch(row[\"phrase_concat\"])[0]\n",
    "    pred_reverse_max_match = \" \".join(tokens_reverse_max_match)\n",
    "    df.at[index, \"pred_reverse_max_match\"] = pred_reverse_max_match\n",
    "    relative_accuracy_reverse_max_match = len(set(real_tokens).intersection(set(tokens_reverse_max_match))) / len(set(real_tokens))\n",
    "    df.at[index, \"relative_accuracy_reverse_max_match\"] = relative_accuracy_reverse_max_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo mixte: \n",
      "16.260162601626014 % de précision exacte\n",
      "68.855959898065 % de mots corrects en moyenne\n",
      "Max match: \n",
      "9.75609756097561 % de précision exacte\n",
      "59.113485868112605 % de mots corrects en moyenne\n",
      "Reverse max match: \n",
      "13.008130081300814 % de précision exacte\n",
      "66.36391498998981 % de mots corrects en moyenne\n"
     ]
    }
   ],
   "source": [
    "mean_algo = sum(df[\"pred_mix_algo\"] == df[\"phrase_espace\"]) / len(df)\n",
    "relative_accuracy_algo = df[\"relative_accuracy_mix_algo\"].mean()\n",
    "mean_max_match = sum(df[\"pred_max_match\"] == df[\"phrase_espace\"]) / len(df)\n",
    "relative_accuracy_max_match = df[\"relative_accuracy_max_match\"].mean()\n",
    "mean_reverse_max_match = sum(df[\"pred_reverse_max_match\"] == df[\"phrase_espace\"]) / len(df)\n",
    "relative_accuracy_reverse_max_match = df[\"relative_accuracy_reverse_max_match\"].mean()\n",
    "print(\"Algo mixte: \")\n",
    "print(mean_algo * 100, \"%\", \"de précision exacte\")\n",
    "print(relative_accuracy_algo * 100, \"%\", \"de mots corrects en moyenne\")\n",
    "print(\"Max match: \")\n",
    "print(mean_max_match * 100, \"%\", \"de précision exacte\")\n",
    "print(relative_accuracy_max_match * 100, \"%\", \"de mots corrects en moyenne\")\n",
    "print(\"Reverse max match: \")\n",
    "print(mean_reverse_max_match * 100, \"%\", \"de précision exacte\")\n",
    "print(relative_accuracy_reverse_max_match * 100, \"%\", \"de mots corrects en moyenne\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
